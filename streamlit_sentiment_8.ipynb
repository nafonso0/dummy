{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5ba5a83-4a03-44e7-964d-4e23b656e4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/nunoafonso/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import io\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "italian_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adff1056-60f5-47a0-91c0-8c1059ff7ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 09:01:38.588 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "st.set_page_config(page_title='Sentiment analysis',  layout='wide', page_icon=':rocket:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68b2cd50-aa45-4567-b403-ce6d61b0c8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 09:01:39.408 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:39.409 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up RoBERTa sentiment pipeline\n",
    "# roberta_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "st.title(\"Sentiment Analysis Tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6058e6b7-705b-4ed8-b1cb-5247e7200250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 09:01:39.874 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:39.885 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:39.885 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:39.886 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:39.887 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:39.887 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:39.888 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:39.888 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:39.889 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:39.889 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:39.890 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Sidebar: Data Input\n",
    "st.sidebar.image(\"logo.png\", width=380)  # Adjust width if needed\n",
    "st.sidebar.header(\"Data Input\")\n",
    "data_source = st.sidebar.radio(\"Select input source:\", [\"Upload CSV\", \"Use Twitter Handle\", \"Reddit Subreddit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "369c2c57-e552-4fdb-8de8-42283c3402ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 09:01:40.368 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:40.369 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:40.369 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:40.370 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 09:01:40.370 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "if data_source == \"Upload CSV\":\n",
    "    uploaded_file = st.sidebar.file_uploader(\"Upload CSV with 'text' and optional 'date' column:\", type=\"csv\")\n",
    "    if uploaded_file:\n",
    "        df = pd.read_csv(uploaded_file)\n",
    "\n",
    "elif data_source == \"Use Twitter Handle\":\n",
    "    try:\n",
    "        import tweepy\n",
    "        st.sidebar.markdown(\"**Twitter API Credentials**\")\n",
    "        api_key = st.sidebar.text_input(\"API Key\")\n",
    "        api_secret = st.sidebar.text_input(\"API Secret\", type=\"password\")\n",
    "        access_token = st.sidebar.text_input(\"Access Token\")\n",
    "        access_secret = st.sidebar.text_input(\"Access Token Secret\", type=\"password\")\n",
    "\n",
    "        handle = st.sidebar.text_input(\"Enter Twitter handle (without @):\")\n",
    "        tweet_limit = st.sidebar.slider(\"Number of recent tweets\", 10, 1000, 100)\n",
    "\n",
    "        if handle and api_key and api_secret and access_token and access_secret:\n",
    "            auth = tweepy.OAuth1UserHandler(api_key, api_secret, access_token, access_secret)\n",
    "            api = tweepy.API(auth)\n",
    "\n",
    "            tweets = []\n",
    "            for tweet in tweepy.Cursor(api.user_timeline, screen_name=f\"@{handle}\", tweet_mode=\"extended\").items(tweet_limit):\n",
    "                tweets.append([tweet.created_at, tweet.full_text])\n",
    "\n",
    "            df = pd.DataFrame(tweets, columns=['date', 'text'])\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading Twitter data: {e}\")\n",
    "\n",
    "elif data_source == \"Reddit Subreddit\":\n",
    "    try:\n",
    "        import praw\n",
    "        st.sidebar.markdown(\"**Reddit API Credentials**\")\n",
    "        reddit_client_id = st.sidebar.text_input(\"Client ID\", value=\"7GwE5Bdr9d6S6zughGa8rA\")\n",
    "        reddit_client_secret = st.sidebar.text_input(\"Client Secret\", value=\"Ng-aZ0kN8ZNdvTBRwDU9ntPd9yD-rA\")\n",
    "        reddit_user_agent = st.sidebar.text_input(\"User Agent\", value=\"sentiment-analysis-script\")\n",
    "\n",
    "        subreddit_name = st.sidebar.text_input(\"Enter subreddit name (without /r/):\")\n",
    "        post_limit = st.sidebar.slider(\"Number of posts\", 10, 500, 100)\n",
    "\n",
    "        if subreddit_name and reddit_client_id and reddit_client_secret and reddit_user_agent:\n",
    "            reddit = praw.Reddit(client_id=reddit_client_id,\n",
    "                                 client_secret=reddit_client_secret,\n",
    "                                 user_agent=reddit_user_agent)\n",
    "\n",
    "            posts = []\n",
    "            for post in reddit.subreddit(subreddit_name).new(limit=post_limit):\n",
    "                # Get the post text\n",
    "                post_text = post.title + \" \" + post.selftext\n",
    "                post_data = [datetime.fromtimestamp(post.created_utc), post_text]\n",
    "                posts.append(post_data)\n",
    "                \n",
    "                # Fetch comments and analyze them as well\n",
    "                post.comments.replace_more(limit=0)  # Replace \"MoreComments\" object\n",
    "                for comment in post.comments.list():\n",
    "                    comment_data = [datetime.fromtimestamp(comment.created_utc), comment.body]\n",
    "                    posts.append(comment_data)\n",
    "\n",
    "            df = pd.DataFrame(posts, columns=['date', 'text'])\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading Reddit data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca826f81-d8f6-4d5f-b35b-3d86f7ff95c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 16:51:08.267 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 16:51:08.269 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# If data is loaded\n",
    "if not df.empty:\n",
    "    if 'text' not in df.columns:\n",
    "        st.error(\"Data must include a 'text' column.\")\n",
    "    else:\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            df.dropna(subset=['date'], inplace=True)\n",
    "            min_date, max_date = df['date'].min(), df['date'].max()\n",
    "            selected_range = st.sidebar.date_input(\"Date range:\", [min_date, max_date])\n",
    "\n",
    "            df = df[(df['date'] >= pd.to_datetime(selected_range[0])) &\n",
    "                    (df['date'] <= pd.to_datetime(selected_range[1]))]\n",
    "\n",
    "        st.sidebar.header(\"2. Sentiment Settings\")\n",
    "        model_choice = st.sidebar.selectbox(\"Choose sentiment model:\", [\"RoBERTa\", \"VADER\", \"Feel-IT\"])\n",
    "        conf_threshold = st.sidebar.slider(\"Confidence threshold\", 0.0, 1.0, 0.5, 0.01)\n",
    "\n",
    "\n",
    "        \n",
    "        with st.spinner(\"Running Sentiment Analysis...\"):\n",
    "            MAX_LENGTH = 512\n",
    "            df['text'] = df['text'].astype(str).apply(lambda x: x[:MAX_LENGTH])\n",
    "    \n",
    "            if model_choice == \"RoBERTa\":\n",
    "                roberta_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "                results = roberta_pipeline(df['text'].tolist())\n",
    "                df['label'] = [r['label'] for r in results]\n",
    "                df['score'] = [r['score'] for r in results]\n",
    "    \n",
    "                df = df[df['score'] >= conf_threshold]\n",
    "    \n",
    "                label_map = {\"LABEL_2\": \"POSITIVE\", \"LABEL_1\": \"NEUTRAL\", \"LABEL_0\": \"NEGATIVE\"}\n",
    "                score_map = {\"POSITIVE\": 1, \"NEUTRAL\": 0, \"NEGATIVE\": -1}\n",
    "    \n",
    "                df['sentiment'] = df['label'].map(label_map)\n",
    "                df['sentiment_score'] = df['sentiment'].map(score_map)\n",
    "    \n",
    "            \n",
    "            \n",
    "            elif model_choice == \"VADER\":\n",
    "                vader = SentimentIntensityAnalyzer()\n",
    "                df['score'] = df['text'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
    "                df = df[df['score'].abs() >= conf_threshold]\n",
    "    \n",
    "                def vader_label(score):\n",
    "                    if score >= 0.05:\n",
    "                        return \"POSITIVE\"\n",
    "                    elif score <= -0.05:\n",
    "                        return \"NEGATIVE\"\n",
    "                    else:\n",
    "                        return \"NEUTRAL\"\n",
    "    \n",
    "                df['sentiment'] = df['score'].apply(vader_label)\n",
    "                score_map = {\"POSITIVE\": 1, \"NEUTRAL\": 0, \"NEGATIVE\": -1}\n",
    "                df['sentiment_score'] = df['sentiment'].map(score_map)\n",
    "\n",
    "\n",
    "            elif model_choice == \"Feel-IT\":\n",
    "                classifier = pipeline(\"text-classification\", model=\"MilaNLProc/feel-it-italian-sentiment\", top_k=1)\n",
    "                results = classifier(df['text'].tolist())\n",
    "                \n",
    "                df['label'] = [r[0]['label'].upper() for r in results]  # Get top prediction and make uppercase\n",
    "                df['score'] = [r[0]['score'] for r in results]\n",
    "            \n",
    "                df = df[df['score'] >= conf_threshold]  # Filter by confidence threshold\n",
    "            \n",
    "                # Only map POSITIVE and NEGATIVE\n",
    "                label_map = {\"POSITIVE\": \"POSITIVE\", \"NEGATIVE\": \"NEGATIVE\"}\n",
    "                score_map = {\"POSITIVE\": 1, \"NEGATIVE\": -1}\n",
    "            \n",
    "                df['sentiment'] = df['label'].map(label_map)\n",
    "                df['sentiment_score'] = df['sentiment'].map(score_map)\n",
    "\n",
    "        \n",
    "        \n",
    "        st.subheader(\"Summary\")\n",
    "        df['sentiment'] = df['sentiment'].fillna('UNKNOWN').astype(str).str.upper().str.strip()\n",
    "        # df['sentiment'] = df['sentiment'].astype(str)\n",
    "        sentiment_counts = df['sentiment'].value_counts().reset_index()\n",
    "        sentiment_counts.columns = ['Sentiment', 'Count']\n",
    "        # sentiment_counts['Sentiment'] = sentiment_counts['Sentiment'].astype(str)  # Just to be extra safe\n",
    "        \n",
    "        col1, col2 = st.columns([1, 1])\n",
    "        \n",
    "        \n",
    "        with col1:\n",
    "            overall_score = df['sentiment_score'].mean()\n",
    "            st.markdown(\n",
    "                f\"\"\"\n",
    "                <div style=\"text-align: center; padding-top: 50px;\">\n",
    "                    <h3>Overall Sentiment Score</h3>\n",
    "                    <div style=\"font-size: 48px; font-weight: bold;\">{overall_score:.2f}</div>\n",
    "                    <div style=\"font-size: 14px; color: gray;\">(Ranges from -1 = very negative to +1 = very positive)</div>\n",
    "                </div>\n",
    "                \"\"\", unsafe_allow_html=True\n",
    "            )\n",
    "                \n",
    "        with col2:\n",
    "            fig = px.pie(sentiment_counts,\n",
    "                         names='Sentiment',\n",
    "                         values='Count',\n",
    "                         title='Sentiment Distribution',\n",
    "                         color='Sentiment',\n",
    "                         color_discrete_map={'POSITIVE': 'green', 'NEUTRAL': 'gray', 'NEGATIVE': 'red'},\n",
    "                         category_orders={\"Sentiment\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]})\n",
    "            fig.update_layout(height=400, margin=dict(t=30, b=0, l=0, r=0)) # Set pie chart height\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        \n",
    "        # st.subheader(\"Summary\")\n",
    "        # sentiment_counts = df['sentiment'].value_counts().reset_index()\n",
    "        # sentiment_counts.columns = ['Sentiment', 'Count']\n",
    "        # st.plotly_chart(px.pie(sentiment_counts, names='Sentiment', values='Count', title='Sentiment Distribution',\n",
    "        #       color='Sentiment', color_discrete_map={'POSITIVE': 'green','NEUTRAL': 'gray','NEGATIVE': 'red'}))\n",
    "\n",
    "        # overall_score = df['sentiment_score'].mean()\n",
    "        # st.metric(\"Overall Sentiment Score ‚ÑπÔ∏è\", f\"{overall_score:.2f}\", help=\"The average of sentiment scores: +1 for Positive, 0 for Neutral, -1 for Negative. Ranges from -1 (very negative) to +1 (very positive).\")\n",
    "        \n",
    "        if 'date' in df.columns:\n",
    "            time_series = df.groupby([df['date'].dt.date, 'sentiment']).size().reset_index(name='Count')\n",
    "            st.subheader(\"Sentiment Over Time\")\n",
    "            st.plotly_chart(px.line(time_series, x='date', y='Count', color='sentiment', title=\"Sentiment Trends Over Time\",\n",
    "              color_discrete_map={'POSITIVE': 'green','NEUTRAL': 'gray','NEGATIVE': 'red'},\n",
    "              category_orders={\"sentiment\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]}))\n",
    "\n",
    "        st.subheader(\"Word Cloud by Sentiment\")\n",
    "\n",
    "        # Ensure necessary NLTK downloads\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('wordnet')\n",
    "        \n",
    "        # Initialize lemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "        \n",
    "        # Function to clean and preprocess the text\n",
    "        def clean_text(text):\n",
    "            text = re.sub(r'[^a-zA-Z√†√®√©√¨√≤√π\\s]', '', text)  # Allow Italian accented letters\n",
    "            words = text.lower().split()\n",
    "            words = [word for word in words if word not in italian_stopwords and len(word) > 2]\n",
    "            return ' '.join(words)\n",
    "        \n",
    "        # Only show POSITIVE and NEGATIVE side by side\n",
    "        cols = st.columns(2)\n",
    "        for idx, sentiment in enumerate(['POSITIVE', 'NEGATIVE']):\n",
    "            # Clean the text before generating word cloud\n",
    "            texts = \" \".join(df[df['sentiment'] == sentiment]['text'])\n",
    "            cleaned_texts = clean_text(texts)  \n",
    "            \n",
    "            # Apply cleaning function\n",
    "            if cleaned_texts:\n",
    "                wordcloud = WordCloud(width=800, height=300, background_color='white').generate(cleaned_texts)\n",
    "        \n",
    "                with cols[idx]:\n",
    "                    st.markdown(f\"#### {sentiment}\")\n",
    "                    fig, ax = plt.subplots(figsize=(10, 3))\n",
    "                    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "                    ax.axis('off')\n",
    "                    st.pyplot(fig)\n",
    "        \n",
    "                    # Extract top 10 words from the word cloud\n",
    "                    word_freq = wordcloud.words_\n",
    "                    top_words = list(word_freq.items())[:10]\n",
    "\n",
    "                    if top_words:\n",
    "                        words, freqs = zip(*top_words)\n",
    "                        \n",
    "                        df_bar = pd.DataFrame({'Word': words, 'Frequency': freqs})\n",
    "\n",
    "                        fig = px.bar(\n",
    "                            df_bar,\n",
    "                            x='Frequency',\n",
    "                            y='Word',\n",
    "                            orientation='h',\n",
    "                            title=f\"Top Words in {sentiment} Posts\",\n",
    "                            color='Frequency',\n",
    "                            color_continuous_scale='Blues'\n",
    "                        )\n",
    "                    \n",
    "                        fig.update_layout(yaxis={'categoryorder': 'total ascending'}, showlegend=False)\n",
    "                        fig.update_coloraxes(showscale=False) # Keep bars sorted\n",
    "                        st.plotly_chart(fig, use_container_width=True)\n",
    "                        \n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "                    # if top_words:\n",
    "                    #     words, freqs = zip(*top_words)\n",
    "                    #     fig_bar, ax_bar = plt.subplots(figsize=(10, 5))\n",
    "                    #     import seaborn as sns\n",
    "                    #     sns.barplot(x=list(freqs), y=list(words), ax=ax_bar, palette=\"Blues_d\")\n",
    "                    #     ax_bar.set_title(f\"Top Words in {sentiment} Posts\")\n",
    "                    #     ax_bar.set_xlabel(\"Relative Frequency\")\n",
    "                    #     fig_bar.tight_layout()  # ensure consistent padding/margins\n",
    "                    #     st.pyplot(fig_bar)\n",
    "\n",
    "        \n",
    "        with st.expander(\"üîç View Analyzed Posts\", expanded=False):\n",
    "            display_df = df[['date', 'text', 'sentiment', 'score']].copy()\n",
    "            display_df.columns = ['Date', 'Text', 'Sentiment', 'Confidence']\n",
    "            st.dataframe(display_df.sort_values(by='Date', ascending=False).reset_index(drop=True), height=400)\n",
    "\n",
    "        st.subheader(\"Download Results\")\n",
    "        csv = df.to_csv(index=False).encode('utf-8')\n",
    "        st.download_button(\"Download CSV\", csv, \"sentiment_results.csv\", \"text/csv\")\n",
    "else:\n",
    "    st.info(\"Awaiting data input...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eca918-176d-4a93-9934-085dbb624a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5a7211a-ce6e-409b-af63-7806d3d32788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlm_roberta_pipeline = pipeline(\"sentiment-analysis\", model=\"xlm-roberta-base\")\n",
    "# result = xlm_roberta_pipeline(\"Oggi sono proprio contento!\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "feeda6b3-df28-4e11-922c-3e5fb79b751b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'positive', 'score': 0.9997411370277405}, {'label': 'negative', 'score': 0.00025880217435769737}]]\n"
     ]
    }
   ],
   "source": [
    "# classifier = pipeline(\"text-classification\", model=\"MilaNLProc/feel-it-italian-sentiment\", top_k=2)\n",
    "# prediction = classifier(\"Oggi sono proprio contento!\")\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c60e3d1c-61f3-46a4-88e9-4293f3e865d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'negative', 'score': 0.9997252821922302}, {'label': 'positive', 'score': 0.0002747138496488333}]]\n"
     ]
    }
   ],
   "source": [
    "# classifier = pipeline(\"text-classification\", model=\"MilaNLProc/feel-it-italian-sentiment\", top_k=2)\n",
    "# prediction = classifier(\"Dice la stessa cosa mio marito sviluppatore. Non ha mai finito la laurea, si √® messo a lavorare. In facolt√† manco gli facevano aprire un PC, insegnavano linguaggi vecchissimi e alcuni esami di programmazione li dovevano fare su carta e penna, che fa gi√† ridere cos√¨.\")\n",
    "# print(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2b398a5-cf15-4adc-9472-9f7ad47c2ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting it-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: it-core-news-sm\n",
      "Successfully installed it-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('it_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7555b-9000-4971-badd-243ab8f20b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
